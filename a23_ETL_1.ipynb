{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16300e7-e0ff-4fd6-b0cc-4985a7fe5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261c4a5b-8b97-4218-a570-a42a560c8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Brazil States CSVs to DataFrame\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1fe9b3-c5ad-417c-bedc-42865c5de470",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    'Alagoas': [\"csv_a23/PAAL2305.csv\"], \n",
    "    'Amazonas': [\"csv_a23/PAAM2303.csv\", \"csv_a23/PAAM2309.csv\", \"csv_a23/PAAM2311.csv\"],\n",
    "    'Bahia': [\"csv_a23/PABA2308.csv\", \"csv_a23/PABA2309.csv\", \"csv_a23/PABA2311.csv\", \"csv_a23/PABA2312.csv\"],\n",
    "    'Distrito Federal': [\"csv_a23/PADF2304.csv\"],\n",
    "    'Pará': [\"csv_a23/PAPA2311.csv\"], \n",
    "    'Roraima': [\"csv_a23/PARR2310.csv\"], \n",
    "    \"São Paulo\": [\"csv_a23/PASP2301a.csv\", \"csv_a23/PASP2301b.csv\", \"csv_a23/PASP2301c.csv\",\n",
    "                  \"csv_a23/PASP2302a.csv\", \"csv_a23/PASP2302b.csv\", \"csv_a23/PASP2302c.csv\",\n",
    "                  \"csv_a23/PASP2304a.csv\", \"csv_a23/PASP2304b.csv\", \"csv_a23/PASP2304c.csv\",\n",
    "                  \"csv_a23/PASP2306a.csv\", \"csv_a23/PASP2306b.csv\", \"csv_a23/PASP2306c.csv\"\n",
    "                 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904950b4-60ea-4440-8a07-a3158884cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['PA_CODUNI', \n",
    "                   'PA_GESTAO',\n",
    "                   'PA_UFMUN',\n",
    "                   'PA_TPUPS',\n",
    "                   'PA_TIPPRE',\n",
    "                   'PA_MVM',\n",
    "                   'PA_CMP',\n",
    "                   'PA_PROC_ID', \n",
    "                   'PA_CNSMED',\n",
    "                   'PA_CBOCOD',\n",
    "                   'PA_CIDPRI',\n",
    "                   'PA_IDADE', \n",
    "                   'PA_SEXO',\n",
    "                   'PA_RACACOR',\n",
    "                   'PA_MUNPCN',\n",
    "                   'PA_QTDPRO',\n",
    "                   'PA_QTDAPR',\n",
    "                   'PA_VALPRO',\n",
    "                   'PA_UFDIF',\n",
    "                   'PA_MNDIF',\n",
    "                   'PA_VALAPR'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f22fd3-de61-4228-af1c-834f96bbc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 'Alagoas' processed.\n",
      "State 'Amazonas' processed.\n",
      "State 'Bahia' processed.\n",
      "State 'Distrito Federal' processed.\n",
      "State 'Pará' processed.\n",
      "State 'Roraima' processed.\n",
      "State 'São Paulo' processed.\n"
     ]
    }
   ],
   "source": [
    "all_states_df = None\n",
    "\n",
    "for state, paths in file_paths.items():\n",
    "    state_df = None\n",
    "    for path in paths:\n",
    "        # Read each CSV file\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Ensure all columns_to_keep are present, add missing ones with null values\n",
    "        existing_columns = df.columns\n",
    "        missing_columns = [c for c in columns_to_keep if c not in existing_columns]\n",
    "        for column in missing_columns:\n",
    "            df = df.withColumn(column, lit(None))\n",
    "        \n",
    "        # Select the necessary columns\n",
    "        df = df.select([col(column) for column in columns_to_keep])\n",
    "        \n",
    "        # Filter the rows where PA_CIDPRI equals 'A23' or fill with NA if no such rows exist\n",
    "        if df.filter(col(\"PA_CIDPRI\") == 'A23').count() > 0:\n",
    "            df = df.filter(col(\"PA_CIDPRI\") == 'A23')\n",
    "        else:\n",
    "            # Create a DataFrame with a single row of 'NA' for columns to keep if PA_CIDPRI = 'A23' does not exist\n",
    "            na_values = ['NA'] * len(columns_to_keep)\n",
    "            na_row = spark.createDataFrame([na_values], columns_to_keep)\n",
    "            df = na_row\n",
    "        \n",
    "        # Add a new column for the state\n",
    "        df = df.withColumn(\"State\", lit(state))\n",
    "        \n",
    "        # Union the dataframes for the same state\n",
    "        if state_df is None:\n",
    "            state_df = df\n",
    "        else:\n",
    "            state_df = state_df.unionByName(df, allowMissingColumns=True)\n",
    "    \n",
    "    # Print the progress indicator\n",
    "    print(f\"State '{state}' processed.\")\n",
    "    \n",
    "    # Union the dataframes for all states\n",
    "    if all_states_df is None:\n",
    "        all_states_df = state_df\n",
    "    else:\n",
    "        all_states_df = all_states_df.unionByName(state_df, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0c46da-79aa-40e9-95b8-fe61623000bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+--------+---------+------+------+----------+---------------+---------+---------+--------+-------+----------+---------+---------+---------+---------+--------+--------+---------+-------+\n",
      "|PA_CODUNI|PA_GESTAO|PA_UFMUN|PA_TPUPS|PA_TIPPRE|PA_MVM|PA_CMP|PA_PROC_ID|      PA_CNSMED|PA_CBOCOD|PA_CIDPRI|PA_IDADE|PA_SEXO|PA_RACACOR|PA_MUNPCN|PA_QTDPRO|PA_QTDAPR|PA_VALPRO|PA_UFDIF|PA_MNDIF|PA_VALAPR|  State|\n",
      "+---------+---------+--------+--------+---------+------+------+----------+---------------+---------+---------+--------+-------+----------+---------+---------+---------+---------+--------+--------+---------+-------+\n",
      "|  3065383|   270240|  270240|      36|        0|  null|  null| 301040044|706705528339119|   223905|      A23|       6|      M|         3|   270240|        1|        1|     2.81|       0|       0|     2.81|Alagoas|\n",
      "|  3065383|   270240|  270240|      36|        0|  null|  null| 301040044|706705528339119|   223905|      A23|       7|      M|         3|   270240|        1|        1|     2.81|       0|       0|     2.81|Alagoas|\n",
      "|  3065383|   270240|  270240|      36|        0|  null|  null| 301040044|706705528339119|   223905|      A23|       7|      M|         3|   270240|        1|        1|     2.81|       0|       0|     2.81|Alagoas|\n",
      "|  3065383|   270240|  270240|      36|        0|  null|  null| 301040044|706705528339119|   223905|      A23|       2|      M|         3|   270240|        1|        1|     2.81|       0|       0|     2.81|Alagoas|\n",
      "|  3065383|   270240|  270240|      36|        0|  null|  null| 301040044|706705528339119|   223905|      A23|       5|      M|         3|   270240|        1|        1|     2.81|       0|       0|     2.81|Alagoas|\n",
      "+---------+---------+--------+--------+---------+------+------+----------+---------------+---------+---------+--------+-------+----------+---------+---------+---------+---------+--------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows of the results\n",
    "all_states_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ade6e2-7ca4-4a13-8ff1-fde53fb55879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "all_states_pandas_df = all_states_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc241f6-008d-4f87-8f93-34f36dc54657",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states_pandas_df.to_csv('csv/a23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e86c7ed5-8e85-48a1-80bd-36b127094003",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
